{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import string\n",
    "import re \n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/wiki10/text_sample/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(path):\n",
    "    return re.search(r'(?<=/)\\w*$', path).group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One RDD element per line in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2728\n",
      "Undeclared was an American television series that aired on Fox during the 2001–2002 season.  The hal\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(DATA_DIR)\n",
    "print(rdd.count())\n",
    "print(rdd.first()[:100])\n",
    "#rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One RDD element per text file, also preserves file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "('011847f83eba74ae537e88609355b068', 'Undeclared was an American television series that aired on Fox during the 2001–2002 season.  The half-hour comedy-drama/sitcom was Judd Apatow\\'s follow-up to the TV cult classic Freaks and Geeks, which also lasted for one season. Undeclared centered on a group of college freshmen at the fictional University of North Eastern California. Unlike Freaks, it was set in the current time of the early 2000s rather than the 1980s. It gave a humorous look at the highs and lows of college life, from young adult relationships to the dreaded freshman fifteen. It takes its name from the status of an undergraduate who has not yet decided, or \"declared\", a specific major of study they wish to take.  College is \"the reward for surviving high school. Most people have great fun stories from college and nightmare stories from high school,\" Judd Apatow told the Los Angeles Times in 2006. He also speculated on why college shows find it hard to gain a foothold on network schedules: \"One reason for the death of college shows is that it\\'s difficult to be honest about campus life on network or basic cable. It\\'s hard to portray truthfully. The truth is, kids are high, drunk and having sex. No matter what you do, you\\'re fudging it.\"[1] Media outlets such as Entertainment Weekly gave the show generally glowing reviews, though the general audience did not seem to share the same opinion of the show, as Fox canceled the show in March 2002 after poor ratings. In total, 15 episodes were shown on Fox, while two unaired episodes were included in the DVD set released in the United States on August 16, 2005.       On the DVD, the episodes were ordered by production number. However, according to Judd Apatow, this was a mistake.[2] The preferred viewing order (with the story in chronological order) is as follows:  On August 16, 2005, Shout! Factory released the complete series of Undeclared on DVD in Region 1. The four-disc boxed set contains special features such as an unaired episode and the script for an un-filmed episode.    ')\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.wholeTextFiles(DATA_DIR).map(lambda x: (extract_filename(x[0]), x[1])) # strip out full path of filename\n",
    "\n",
    "document_count = rdd.count()\n",
    "\n",
    "print(document_count)\n",
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.sub(f'([{string.punctuation}])', r' \\1 ', text.lower()).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenized RDD containing one element per document. Each element is a (filename, [list of tokens]) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('011847f83eba74ae537e88609355b068', ['undeclared', 'was', 'an', 'american', 'television', 'series', 'that', 'aired', 'on', 'fox', 'during', 'the', '2001–2002', 'season', '.', 'the', 'half', '-', 'hour', 'comedy', '-', 'drama', '/', 'sitcom', 'was', 'judd', 'apatow', \"'\", 's', 'follow', '-', 'up', 'to', 'the', 'tv', 'cult', 'classic', 'freaks', 'and', 'geeks', ',', 'which', 'also', 'lasted', 'for', 'one', 'season', '.', 'undeclared', 'centered', 'on', 'a', 'group', 'of', 'college', 'freshmen', 'at', 'the', 'fictional', 'university', 'of', 'north', 'eastern', 'california', '.', 'unlike', 'freaks', ',', 'it', 'was', 'set', 'in', 'the', 'current', 'time', 'of', 'the', 'early', '2000s', 'rather', 'than', 'the', '1980s', '.', 'it', 'gave', 'a', 'humorous', 'look', 'at', 'the', 'highs', 'and', 'lows', 'of', 'college', 'life', ',', 'from', 'young', 'adult', 'relationships', 'to', 'the', 'dreaded', 'freshman', 'fifteen', '.', 'it', 'takes', 'its', 'name', 'from', 'the', 'status', 'of', 'an', 'undergraduate', 'who', 'has', 'not', 'yet', 'decided', ',', 'or', '\"', 'declared', '\"', ',', 'a', 'specific', 'major', 'of', 'study', 'they', 'wish', 'to', 'take', '.', 'college', 'is', '\"', 'the', 'reward', 'for', 'surviving', 'high', 'school', '.', 'most', 'people', 'have', 'great', 'fun', 'stories', 'from', 'college', 'and', 'nightmare', 'stories', 'from', 'high', 'school', ',', '\"', 'judd', 'apatow', 'told', 'the', 'los', 'angeles', 'times', 'in', '2006', '.', 'he', 'also', 'speculated', 'on', 'why', 'college', 'shows', 'find', 'it', 'hard', 'to', 'gain', 'a', 'foothold', 'on', 'network', 'schedules', ':', '\"', 'one', 'reason', 'for', 'the', 'death', 'of', 'college', 'shows', 'is', 'that', 'it', \"'\", 's', 'difficult', 'to', 'be', 'honest', 'about', 'campus', 'life', 'on', 'network', 'or', 'basic', 'cable', '.', 'it', \"'\", 's', 'hard', 'to', 'portray', 'truthfully', '.', 'the', 'truth', 'is', ',', 'kids', 'are', 'high', ',', 'drunk', 'and', 'having', 'sex', '.', 'no', 'matter', 'what', 'you', 'do', ',', 'you', \"'\", 're', 'fudging', 'it', '.', '\"', '[', '1', ']', 'media', 'outlets', 'such', 'as', 'entertainment', 'weekly', 'gave', 'the', 'show', 'generally', 'glowing', 'reviews', ',', 'though', 'the', 'general', 'audience', 'did', 'not', 'seem', 'to', 'share', 'the', 'same', 'opinion', 'of', 'the', 'show', ',', 'as', 'fox', 'canceled', 'the', 'show', 'in', 'march', '2002', 'after', 'poor', 'ratings', '.', 'in', 'total', ',', '15', 'episodes', 'were', 'shown', 'on', 'fox', ',', 'while', 'two', 'unaired', 'episodes', 'were', 'included', 'in', 'the', 'dvd', 'set', 'released', 'in', 'the', 'united', 'states', 'on', 'august', '16', ',', '2005', '.', 'on', 'the', 'dvd', ',', 'the', 'episodes', 'were', 'ordered', 'by', 'production', 'number', '.', 'however', ',', 'according', 'to', 'judd', 'apatow', ',', 'this', 'was', 'a', 'mistake', '.', '[', '2', ']', 'the', 'preferred', 'viewing', 'order', '(', 'with', 'the', 'story', 'in', 'chronological', 'order', ')', 'is', 'as', 'follows', ':', 'on', 'august', '16', ',', '2005', ',', 'shout', '!', 'factory', 'released', 'the', 'complete', 'series', 'of', 'undeclared', 'on', 'dvd', 'in', 'region', '1', '.', 'the', 'four', '-', 'disc', 'boxed', 'set', 'contains', 'special', 'features', 'such', 'as', 'an', 'unaired', 'episode', 'and', 'the', 'script', 'for', 'an', 'un', '-', 'filmed', 'episode', '.'])\n"
     ]
    }
   ],
   "source": [
    "rdd_tokens = rdd.map(lambda x: (x[0], tokenize(x[1]))).persist() # cache result as we will be using is multiple times\n",
    "\n",
    "print(rdd_tokens.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create term frequency RDD with one element per token, document pair. Eache element is a (token, (term frequency, filename)) tuple, where term frequency is the number of times that token appears in that document. Arranged this way to be able to join with document frequency RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('undeclared', (3, '011847f83eba74ae537e88609355b068')), ('was', (4, '011847f83eba74ae537e88609355b068')), ('an', (4, '011847f83eba74ae537e88609355b068')), ('american', (1, '011847f83eba74ae537e88609355b068')), ('television', (1, '011847f83eba74ae537e88609355b068'))]\n"
     ]
    }
   ],
   "source": [
    "rdd_tf = rdd_tokens.flatMap(lambda x: [(token, (count, x[0])) for token, count in Counter(x[1]).items()])\n",
    "\n",
    "rdd_tf.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document frequency RDD with one element per token. Each element is a (token, document frequency) tuple where document frequency is the proportion of documents that token appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 0.819),\n",
       " ('name', 0.396),\n",
       " ('share', 0.12),\n",
       " ('angeles', 0.067),\n",
       " ('for', 0.964)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df = (rdd_tokens.flatMap(lambda x: set(x[1]))\n",
    "                    .map(lambda x: (x, 1))\n",
    "                    .reduceByKey(lambda x, y: x + y) \n",
    "                    .map(lambda x: (x[0], x[1] / document_count)))\n",
    "rdd_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('undeclared', ((3, '011847f83eba74ae537e88609355b068'), 0.002)),\n",
       " ('undeclared', ((1, '01bd03069620c8f5e101f9919d24d62d'), 0.002)),\n",
       " ('was', ((4, '011847f83eba74ae537e88609355b068'), 0.819)),\n",
       " ('was', ((15, '0a52ba2347e51f37e0e54e211a6c8db3'), 0.819)),\n",
       " ('was', ((1, '03881942942f39cfeba1c592e355b429'), 0.819))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_join = rdd_tf.join(rdd_df)\n",
    "\n",
    "rdd_join.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(tf, df):\n",
    "    return tf * np.log(1 / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('011847f83eba74ae537e88609355b068', 'undeclared', 18.643824295266576),\n",
       " ('01bd03069620c8f5e101f9919d24d62d', 'undeclared', 6.214608098422191),\n",
       " ('011847f83eba74ae537e88609355b068', 'was', 0.7986847805162709),\n",
       " ('0a52ba2347e51f37e0e54e211a6c8db3', 'was', 2.9950679269360156),\n",
       " ('03881942942f39cfeba1c592e355b429', 'was', 0.19967119512906772)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tf_idf = rdd_join.map(lambda x: (x[1][0][1], x[0], tf_idf(tf=x[1][0][0], df=x[1][1])))\n",
    "\n",
    "rdd_tf_idf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[155] at RDD at PythonRDD.scala:48 []\n",
      " |  MapPartitionsRDD[147] at mapPartitions at PythonRDD.scala:122 []\n",
      " |  ShuffledRDD[146] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[145] at join at <ipython-input-113-2d854cda9982>:1 []\n",
      "    |  PythonRDD[144] at join at <ipython-input-113-2d854cda9982>:1 []\n",
      "    |  UnionRDD[143] at union at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[141] at RDD at PythonRDD.scala:48 []\n",
      "    |  ../../data/wiki10/text_sample/ MapPartitionsRDD[106] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\n",
      "    |  WholeTextFileRDD[105] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[142] at RDD at PythonRDD.scala:48 []\n",
      "    |  MapPartitionsRDD[139] at mapPartitions at PythonRDD.scala:122 []\n",
      "    |  ShuffledRDD[138] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      "    +-(1) PairwiseRDD[137] at reduceByKey at <ipython-input-112-37ff74581df4>:3 []\n",
      "       |  PythonRDD[136] at reduceByKey at <ipython-input-112-37ff74581df4>:3 []\n",
      "       |  ../../data/wiki10/text_sample/ MapPartitionsRDD[106] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\n",
      "       |  WholeTextFileRDD[105] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd_tf_idf.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TF-IDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- filename: string (nullable = false)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark.read.text(DATA_DIR)\n",
    "           .withColumn(\"filename\", F.regexp_extract(F.input_file_name(), r'/(\\w*)$', 1))\n",
    "           .withColumn(\"tokens\", F.split(F.lower(F.col(\"value\")), ' ')) \n",
    "     ) \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               value|            filename|              tokens|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Adolf Hitler in 1...|0937e815f6fac2890...|[adolf, hitler, i...|\n",
      "|Auschwitz-Birkena...|0937e815f6fac2890...|[auschwitz-birken...|\n",
      "|between Israel an...|0937e815f6fac2890...|[between, israel,...|\n",
      "| Quotations from ...|0937e815f6fac2890...|[, quotations, fr...|\n",
      "| Source texts fro...|0937e815f6fac2890...|[, source, texts,...|\n",
      "| Images and media...|0937e815f6fac2890...|[, images, and, m...|\n",
      "| News stories fro...|0937e815f6fac2890...|[, news, stories,...|\n",
      "|Peace treaty sign...|028c3b31da88f3e08...|[peace, treaty, s...|\n",
      "|Stanley Kubrick (...|0238d586d7d6676f9...|[stanley, kubrick...|\n",
      "|Istanbul (Turkish...|0b9f59c213a048d5c...|[istanbul, (turki...|\n",
      "|To the west, to t...|0b9f59c213a048d5c...|[to, the, west,, ...|\n",
      "| Quotations from ...|0b9f59c213a048d5c...|[, quotations, fr...|\n",
      "| Source texts fro...|0b9f59c213a048d5c...|[, source, texts,...|\n",
      "| Images and media...|0b9f59c213a048d5c...|[, images, and, m...|\n",
      "| News stories fro...|0b9f59c213a048d5c...|[, news, stories,...|\n",
      "|Coordinates: 41°0...|0b9f59c213a048d5c...|[coordinates:, 41...|\n",
      "|The Indian Rebell...|00f5eb0a7e5e76360...|[the, indian, reb...|\n",
      "|In sociolinguisti...|00e0a5fe7b7b37034...|[in, sociolinguis...|\n",
      "|         U (to God) |00e0a5fe7b7b37034...|    [u, (to, god), ]|\n",
      "|     አንቺ (anči) (f) |00e0a5fe7b7b37034...|[አንቺ, (anči), (f), ]|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|            filename|           token|\n",
      "+--------------------+----------------+\n",
      "|0937e815f6fac2890...|           adolf|\n",
      "|0937e815f6fac2890...|          hitler|\n",
      "|0937e815f6fac2890...|              in|\n",
      "|0937e815f6fac2890...|            1933|\n",
      "|0937e815f6fac2890...|        pogroms:|\n",
      "|0937e815f6fac2890...| kristallnacht ·|\n",
      "|0937e815f6fac2890...|     bucharest ·|\n",
      "|0937e815f6fac2890...|       dorohoi ·|\n",
      "|0937e815f6fac2890...|          iaşi ·|\n",
      "|0937e815f6fac2890...|        kaunas ·|\n",
      "|0937e815f6fac2890...|      jedwabne ·|\n",
      "|0937e815f6fac2890...|            lviv|\n",
      "|0937e815f6fac2890...|        ghettos:|\n",
      "|0937e815f6fac2890...|        łachwa ·|\n",
      "|0937e815f6fac2890...|          łódź ·|\n",
      "|0937e815f6fac2890...|          lwów ·|\n",
      "|0937e815f6fac2890...|        kraków ·|\n",
      "|0937e815f6fac2890...|      budapest ·|\n",
      "|0937e815f6fac2890...|theresienstadt ·|\n",
      "|0937e815f6fac2890...|         kovno ·|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_long = df.select(\"filename\", F.explode(\"tokens\").alias(\"token\")) \n",
    "\n",
    "df_long.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---+\n",
      "|            filename|       token| tf|\n",
      "+--------------------+------------+---+\n",
      "|0937e815f6fac2890...|     invaded|  6|\n",
      "|0937e815f6fac2890...|          on|178|\n",
      "|0937e815f6fac2890...|    huetler,|  1|\n",
      "|0937e815f6fac2890...|      member|  6|\n",
      "|0937e815f6fac2890...|   countries|  2|\n",
      "|0937e815f6fac2890...|      terms,|  1|\n",
      "|0937e815f6fac2890...|     system\"|  1|\n",
      "|0937e815f6fac2890...| hindenburg.|  1|\n",
      "|0937e815f6fac2890...|        kaas|  1|\n",
      "|0937e815f6fac2890...|      state,|  5|\n",
      "|0937e815f6fac2890...|      repair|  1|\n",
      "|0937e815f6fac2890...|disappointed|  2|\n",
      "|0937e815f6fac2890...|     spanish|  1|\n",
      "|0937e815f6fac2890...|     franco,|  2|\n",
      "|0937e815f6fac2890...|      arises|  1|\n",
      "|0937e815f6fac2890...|    build-up|  1|\n",
      "|0937e815f6fac2890...|     kershaw|  3|\n",
      "|0937e815f6fac2890...|      \"there|  1|\n",
      "|0937e815f6fac2890...|    thoughts|  1|\n",
      "|0937e815f6fac2890...|  tripartite|  2|\n",
      "+--------------------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tf = df_long.groupby(\"filename\", \"token\").agg(F.count(\"*\").alias(\"tf\")) \n",
    "\n",
    "df_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|         token| df|\n",
      "+--------------+---+\n",
      "|    successor.|  3|\n",
      "|         still|417|\n",
      "|   president's| 14|\n",
      "|          hope| 60|\n",
      "|palaestrae.[4]|  1|\n",
      "|        spared|  8|\n",
      "|   expressive.|  1|\n",
      "|         inner| 60|\n",
      "|        filing|  9|\n",
      "|       fossett|  1|\n",
      "|     connected|106|\n",
      "|         1966,| 19|\n",
      "|         1946,| 17|\n",
      "|         those|494|\n",
      "|     traveling| 33|\n",
      "|          zr ·|  2|\n",
      "|         1970s| 67|\n",
      "| infinitesimal|  5|\n",
      "|          some|743|\n",
      "|        width,|  2|\n",
      "+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_df = df_tf.groupby(\"token\").agg(F.count(\"*\").alias(\"df\"))\n",
    "\n",
    "df_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count = df.select(F.countDistinct(\"filename\").alias(\"count\")).first()[\"count\"] \n",
    "\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+\n",
      "|            filename|               token|(tf * LOG((1000 / df)))|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "|08e4da2aa7a77e002...|         \"absolutely|      6.214608098422191|\n",
      "|0c825364f60dce638...|         \"absolutely|      6.214608098422191|\n",
      "|09a2600ed21a3ba87...|        \"all-giving\"|      20.72326583694641|\n",
      "|0bb5a696a98db928c...|\"anti-elitism\"—ac...|      6.907755278982137|\n",
      "|028f984ff7c11ad14...|         \"autopilot\"|      6.907755278982137|\n",
      "|0561d6733af451a4a...|             \"colory|      6.907755278982137|\n",
      "|022fcb4c6dc724080...|         \"composer's|      6.907755278982137|\n",
      "|0053ee7f520dda846...|              \"decal|      6.907755278982137|\n",
      "|075656dc1199b28ec...|         \"disclosure|      6.907755278982137|\n",
      "|0937e815f6fac2890...|      \"encirclement\"|      6.907755278982137|\n",
      "|038ceb44453aa83aa...|              \"grand|      5.115995809754082|\n",
      "|00ce4dcc93e6d1f4d...|              \"grand|     10.231991619508165|\n",
      "|058a0e5f2b1b119fe...|              \"grand|     10.231991619508165|\n",
      "|0947cf680724fdc27...|              \"grand|      5.115995809754082|\n",
      "|07ae7d8c04ab40ba1...|              \"grand|      5.115995809754082|\n",
      "|06a006c47be69b827...|              \"grand|      5.115995809754082|\n",
      "|00c446f54dbb6c155...|          \"guidance\"|      6.907755278982137|\n",
      "|02202f579452164dd...|             \"heroes|      20.72326583694641|\n",
      "|082b89985d42a044f...|       \"lahey-space\"|      6.907755278982137|\n",
      "|0ab614b04a60f8c6c...|              \"less,|      6.907755278982137|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_tf.join(df_df, df_tf.token == df_df.token)\n",
    "      .select(\"filename\", df_tf.token, df_tf.tf * F.log(df_count / df_df.df))).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
